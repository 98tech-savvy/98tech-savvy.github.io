---
title:  "[CS] robots.txt 란 무엇일까?"
excerpt: "robots.txt 총 정리"

//현재 카테고리: Computer, Blog, Python, License
categories:
  - Computer
tags:
  - [Computer, robots, 크롤링]

toc: true
toc_sticky: true

date: 2023-07-03
last_modified_at: 2023-07-03
---

# robots.txt 란?
robots.txt는 웹사이트에서 크롤링하며 정보를 수집하는 검색엔진 크롤러(크롤링 하는 엔진 혹은 검색 로봇)가 액세스 하거나 정보 수집을 해도 되는 페이지가 무엇인지, 해서는 안 되는 페이지가 무엇인지에 대해 알려주는 역할을 하는 텍스트 파일이다. 

# robots.txt 가 필요한 이유
robots.txt가 필요한 이유에는 크게 세 가지가 있다.

1. 과도한 크롤링 및 요청으로 과부하의 방지
크롤러의 정보 수집과 크롤링을 제한함으로써 불필요한 크롤러의 요청(request)을 줄이고 서버에서 처리해야 하는 요청을 줄여 과부하 문제가 생기는 것을 방지할 수 있다.

2. 크롤 버짓(Crawl Budget) 낭비 방지를 위해
크롤 버짓<sup>Crawl Budget</sup>은 검색엔진 크롤러의 일일 요청수를 뜻한다. 이 때 검색엔진이 웹사이트에 방문해 웹 사이트의 규모와 여러가지 요소를 고려해 얼마나 많은 페이지를 방문 및 수집할지 설정한다.

3. 크롤러에게 사이트맵(sitemap.xml)의 위치를 제공
크롤러에게 사이트맵의 위치를 제공해 검색엔진에게 더 잘 발견될 수 있도록 하기 위해 사용하기도 한다. 물론 구글 서치 콘솔/네이버 서치 어드바이저 같은 웹마스터 도구를 이용하면 사이트맵을 검색 엔진에 직접 제출해 노출 빈도를 높힐 수 있지만 robots.txt에 사이트맵 디렉토리를 언급함으로써 구글/네이버를 포함한 다른 검색엔진 크롤러에게 빠르게 발견될 수 있다.

# robots.txt 작성하기
robots 파일은 기본적으로 텍스트파일(.txt)로 되어있다. 어떤 크롤러를 지정할 지, 어떤 디렉토리를 제한할 지에 대한 정보를 적어줄 수 있다.

구성하는 요소는 크게 네 가지로

1. User-agent: robots.txt 에서 지정하는 크롤링 규칙이 적용되어야 할 크롤러를 지정
2. Allow: 크롤링을 허용할 경로 (/상대 경로).
3. Disallow: 크롤링을 제한할 경로 (/상대 경로).
4. Sitemap: 사이트맵이 위치한 경로의 전체 URL (https:// 부터 /sitemap.xml 까지의 전체 절대경로 URL).